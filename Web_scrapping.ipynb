{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web scrapping.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "P3TG77q6g80b"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMDpAKcCh7i3nRx4hQJkWR3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milindkhare/milindkhare/blob/Pypractice/Web_scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3TG77q6g80b"
      },
      "source": [
        "# Error Handling in Py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maOIeFszxqH8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTWfkdp_dab9",
        "outputId": "213e2e33-c060-4885-d94a-9ea88289f1f7"
      },
      "source": [
        "## Error handling\n",
        "for i in range(2):\n",
        "  try:\n",
        "    print(\"1\")\n",
        "    print(2)\n",
        "    print(hey)\n",
        "  #except:\n",
        "  except Exception as e:\n",
        "    print(\"Dude:\", e)\n",
        "\n",
        "print('yo man')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "Dude: name 'hey' is not defined\n",
            "1\n",
            "2\n",
            "Dude: name 'hey' is not defined\n",
            "yo man\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfUShFp0kxQ1"
      },
      "source": [
        "# Web scrapping in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVKqO6ZAJvKV"
      },
      "source": [
        "Using Requests and Beautiful Soup for mining Apple prev close share price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir87EJnQPDt8",
        "outputId": "0782118f-fd6c-4a18-a134-97504383a0a6"
      },
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://finance.yahoo.com/quote/AAPL?p=AAPL\"\n",
        "\n",
        "response = requests.get(url)\n",
        "print(response)\n",
        "\n",
        "  text = response.text\n",
        "\n",
        "  #This is obtained by inspect html of url\n",
        "  prop = \"Previous Close\"\n",
        "\n",
        "# Find index where prop begins\n",
        "prop_ind = text.index(prop) \n",
        "\n",
        "#split till first span and then take out value of prev close\n",
        "#this part is done after looking manually at isnpect html of the page\n",
        "reduced_text = text[prop_ind:].split(\"</span>\")[1]\n",
        "prop_value = reduced_text.split(\">\")[-1]\n",
        "print(prop_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n",
            "148.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj8zsX7jQQ0u"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://finance.yahoo.com/quote/ALLE?p=ALLE\"\n",
        "\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "#print(text)\n",
        "soup = BeautifulSoup(text, features='html.parser')\n",
        "trs = soup.find_all(\"tr\") #This is obtained by inspect html of url\n",
        "print(trs)\n",
        "# print(trs[0].contents[0].text)\n",
        "# print(trs[0].contents[1].text)\n",
        "# print(trs[1].contents[0].text)\n",
        "# print(trs[1].contents[1].text)\n",
        "# print(range(len(trs)))\n",
        "\n",
        "# Retrieve all content for 1 stock\n",
        "\n",
        "#names = []\n",
        "#values = []\n",
        "# nameval = {} # Dictionary of key and value pair (Names can directly used as index for value)\n",
        "\n",
        "for i in range(len(trs)):\n",
        "  for j in range(len(trs[i].contents)):\n",
        "    if j == 0: #first is name\n",
        "      try:\n",
        "        name = trs[i].contents[j].text\n",
        "        #names.append(name)\n",
        "      except:\n",
        "        continue #move on to next block\n",
        "    if j == 1: #second is value\n",
        "      try:\n",
        "        value = trs[i].contents[j].text\n",
        "        #values.append(value)\n",
        "      except:\n",
        "        continue #move on to next block\n",
        "  #add it in dict\n",
        "  nameval[name] = value\n",
        "  if name == '1y Target Est': #final name in the table\n",
        "    break\n",
        "\n",
        "# print(nameval)\n",
        "#print(names) print(values)\n",
        "#names() and values can be used to store vector data seperately if needed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trD2MJtj1Zbz"
      },
      "source": [
        "Fetch all S&P500 stocks data, make it repeated run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fotf7eCz1rxn"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time,os,datetime\n",
        "\n",
        "wikiurl = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "response = requests.get(wikiurl)\n",
        "pagetext = response.text\n",
        "\n",
        "# First we try to find all the ticker symbols from wiki page -----\n",
        "wikisoup = BeautifulSoup(pagetext, features='html.parser')\n",
        "tbody = wikisoup.find_all(\"tbody\") #This is obtained by inspect html of url\n",
        "\n",
        "#print(tbody[0].contents[2])\n",
        "# every even content has ticker symbol\n",
        "\n",
        "## Retrieve all tickers\n",
        "tickersymbols = []\n",
        "endsymbol = 'ZTS'\n",
        "\n",
        "for i in range(len(tbody[0].contents)):\n",
        "  if i<2:\n",
        "    continue\n",
        "  if i%2 != 0:\n",
        "  #if int(i/2)!=i/2:\n",
        "    continue\n",
        "  # fetch symbol\n",
        "  symbol = tbody[0].contents[i].contents[1].text\n",
        "  tickersymbols.append(symbol.strip(\"\\n\"))\n",
        "\n",
        "  if symbol.strip(\"\\n\") == endsymbol: #final name in the table\n",
        "    break\n",
        "\n",
        "print(tickersymbols)\n",
        "\n",
        "# Define function for fetching yahoo fin data -----\n",
        "def getFinancialInformation(symbol):\n",
        "  url = \"https://finance.yahoo.com/quote/\"+symbol+\"?p=\"+symbol+\"\"\n",
        "\n",
        "  response = requests.get(url)\n",
        "  text = response.text\n",
        "  \n",
        "  soup = BeautifulSoup(text, features='html.parser')\n",
        "  trs = soup.find_all(\"tr\") #This is obtained by inspect html of url\n",
        "  \n",
        "  # Retrieve all content for stock\n",
        "  names = []\n",
        "  values = []\n",
        "  #nameval = {} # Dictionary of key and value pair (Names can directly used as index for value)\n",
        "\n",
        "  for i in range(len(trs)):\n",
        "    for j in range(len(trs[i].contents)):\n",
        "      if j == 0: #first is name\n",
        "        try:\n",
        "          name = trs[i].contents[j].text\n",
        "          names.append(name)\n",
        "        except:\n",
        "          continue #move on to next block\n",
        "      if j == 1: #second is value\n",
        "        try:\n",
        "          value = trs[i].contents[j].text\n",
        "          values.append(value)\n",
        "        except:\n",
        "          continue #move on to next block\n",
        "      \n",
        "      #print(name)\n",
        "      #print(\"/n\")\n",
        "    \n",
        "  #add it in dict\n",
        "  #nameval[name] = value\n",
        "    if name == '1y Target Est': #final name in the table\n",
        "     break\n",
        "  \n",
        "  #print(names)\n",
        "  return names,values\n",
        "\n",
        "# Now use ticker symbols to fetch all yahoo fin data -----\n",
        "\n",
        "# and make it run every 15 min\n",
        "# append data\n",
        "# store time stamp of values\n",
        "\n",
        "while True:\n",
        "  # check current data\n",
        "  start = time.time()\n",
        "  reruntime = 900 #15 min\n",
        "\n",
        "  # Extract and save data\n",
        "  data = {\"symbol\":[],\n",
        "          \"metric\":[],\n",
        "          \"value\":[],\n",
        "          \"time\"=[]}\n",
        "\n",
        "  for symbol in tickersymbols:\n",
        "    print(symbol)\n",
        "    try:\n",
        "      names,values = getFinancialInformation(symbol) #retrieve data for symbol\n",
        "      fetchtime = str(datetime.datetime.now())\n",
        "    except Exception as e:\n",
        "      print(\"fetch failed for : \"+symbol)\n",
        "      print(\"error: \"+str(e))\n",
        "      continue\n",
        "\n",
        "    #store each names value as tuple under symbol\n",
        "    for i in range(len(names)):\n",
        "      try:      \n",
        "        data[\"symbol\"].append(symbol)\n",
        "        data[\"metric\"].append(names[i])\n",
        "        data[\"value\"].append(values[i])\n",
        "        data[\"time\"].append(fetchtime)\n",
        "        #print(data)\n",
        "      except:\n",
        "        print(i)\n",
        "\n",
        "    # another way to create data without for loop\n",
        "    #data[\"symbol\"] +=.append(symbol) \n",
        "    #data[\"metric\"] += names\n",
        "    #data[\"value\"] += values\n",
        "\n",
        "  #Create dataframe\n",
        "  df = pd.DataFrame(data)\n",
        "  date = datetime.date.today()\n",
        "  savepath = str(date)+\"S&P data.csv\"\n",
        "  if od.path.isfile(savepath):\n",
        "    #dont overwrite, append\n",
        "    df.to_csv(savepath, mode='a', header=False, columns=[\"symbol\", \"metric\", \"value\"])\n",
        "  else:\n",
        "    #create\n",
        "    df.to_csv(savepath, columns=[\"symbol\", \"metric\", \"value\"])\n",
        "\n",
        "  # wait until rerun time\n",
        "  timediff = time.time() - start\n",
        "  if timediff > reruntime: #900 sec\n",
        "    time.sleep(reruntime)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HYuryBKSAcZ",
        "outputId": "5b730ac1-0100-4d04-8dbe-2406b27462c2"
      },
      "source": [
        "#tickersymbols[0:2]\n",
        "#getFinancialInformation(\"ALLE\")\n",
        "df = pd.read_csv(\"S&P data.csv\", index_col =0)\n",
        "df.head()\n",
        "df.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "symbol              ZTS\n",
              "metric    1y Target Est\n",
              "value            225.00\n",
              "Name: 7151, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeI00LIc6Fmm"
      },
      "source": [
        "Web scrapping Exercise-\n",
        "  url = \"http://webscraper.io/test-sites/tables\" has tables. Extract all #,Fname, Lname and username and put in pd df."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpiYEpJ-6K_v",
        "outputId": "9d8dc537-42d9-4363-9c46-3d82cb840ef6"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "url = \"http://webscraper.io/test-sites/tables\"\n",
        "\n",
        "response = requests.get(url)\n",
        "#print(response.text)#Always visualizing our progress\n",
        "#Each value in the table is surrounded by <tr> </tr>\n",
        "tables = response.text.split(\"<tr>\")\n",
        "counter = 0\n",
        "\n",
        "for table in tables:\n",
        "    if \"Mark\" in table:#Seeing where data tarts\n",
        "        print(\"Mark:\",counter)\n",
        "    if \"@twitter\" in table:#Seeing where data ends\n",
        "        print(\"Twitter:\",counter)\n",
        "    counter+=1\n",
        "    \n",
        "tables = tables[2:19] #Reducing our data down to what's interesting\n",
        "#print(tables)#Always visualizing our progress\n",
        "reducedTables = []\n",
        "for table in tables:\n",
        "    if \"<td>\" in table:\n",
        "        reducedTables.append(table)\n",
        "#print(reducedTables)#Always visualizing our progress\n",
        "\n",
        "doubleReducedTables = []\n",
        "for table in reducedTables:\n",
        "    temp = table.split(\"<td>\")\n",
        "    for tableTemp in temp:\n",
        "        if \"</td>\" in tableTemp:\n",
        "            #Here we make the first part of the string be the #, name, last name, username\n",
        "            doubleReducedTables.append(tableTemp) \n",
        "#print(doubleReducedTables)#Always visualizing our progress\n",
        "#To store out data in a neat fashion\n",
        "data = {\"#\":[],\n",
        "        \"First Name\":[],\n",
        "        \"Last Name\":[],\n",
        "        \"Username\":[]}\n",
        "for i in range(len(doubleReducedTables)):\n",
        "    table = doubleReducedTables[i]\n",
        "    t = i%4 #Cycling over 4 different values\n",
        "    value = table.split(\"</td>\")[0]\n",
        "    #Our data is always before the </td> value, so the first element after the split\n",
        "    if value != \"-\":#We can take out the empty line\n",
        "        if t == 0:\n",
        "            data[\"#\"].append(value)\n",
        "        elif t == 1:\n",
        "            data[\"First Name\"].append(value)\n",
        "        elif t == 2:\n",
        "            data[\"Last Name\"].append(value)\n",
        "        elif t == 3:\n",
        "            data[\"Username\"].append(value)\n",
        "    \n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mark: 2\n",
            "Twitter: 4\n",
            "Mark: 10\n",
            "Twitter: 12\n",
            "Mark: 16\n",
            "Twitter: 18\n",
            "    # First Name Last Name  Username\n",
            "0   1       Mark      Otto      @mdo\n",
            "1   2      Jacob  Thornton      @fat\n",
            "2   3      Larry  the Bird  @twitter\n",
            "3   4      Harry    Potter       @hp\n",
            "4   5       John      Snow    @dunno\n",
            "5   6        Tim      Bean  @timbean\n",
            "6   1       Mark      Otto      @mdo\n",
            "7   2      Jacob  Thornton      @fat\n",
            "8   3      Larry  the Bird  @twitter\n",
            "9   1       Mark      Otto      @mdo\n",
            "10  2      Jacob  Thornton      @fat\n",
            "11  3      Larry  the Bird  @twitter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxtOLgVsxulj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqSUbzCKxy1P"
      },
      "source": [
        "# Scrapping AJAX content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7fMhNJ3xu7p"
      },
      "source": [
        "#url = \"http://testing-ground.scraping.pro/ajax\"\n",
        "#from selenium import webdriver\n",
        "#Try your code here\n",
        "#You can see the solution in the Solution.txt file on the left side\n",
        "\n",
        "url = \"http://testing-ground.scraping.pro/ajax\"\n",
        "from selenium import webdriver\n",
        "\n",
        "def findXPath(element,target,path):\n",
        "    if target in element.get_attribute('textContent') and element.tag_name == \"ul\":\n",
        "        return path\n",
        "    newElements = element.find_elements_by_xpath(\"./*\")\n",
        "    for newElement in newElements:\n",
        "#        print(path+ \"/\"+newElement.tag_name)\n",
        "        final = findXPath(newElement,target,path + \"/\"+newElement.tag_name)\n",
        "        if final != \"\":\n",
        "            return final\n",
        "    return \"\"\n",
        "browser = webdriver.PhantomJS(executable_path='/usr/local/bin/phantomjs')\n",
        "browser.get(url)\n",
        "#print(browser.page_source)\n",
        "elements = browser.find_element_by_xpath(\"html\")\n",
        "finalXPath = findXPath(elements,\"Andrew\",\"html\")\n",
        "print(\"Final xPath:\",finalXPath)\n",
        "element = browser.find_element_by_xpath(finalXPath)\n",
        "print(\"Names:\\n\",element.text) #Fortunately here, we don't need to use get_attribute(\"textContent\")\n",
        "                                 #But you can try it out anyway to see the difference in formatting\n",
        "browser.quit()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}